\documentclass{eceasst}
% This is an empty ECEASST article that can be used as a template
% by authors.
% Just uncomment the appropriate frontmatter commands and provide
% the parameters.

% Required packages
% =================
% Your \usepackage commands go here.

% Volume frontmatter
% ==================
%\input{frontmatter}

% Volume frontmatter for AVoCS 2014
% =====================================
\volume{XXX}{2014} % Volume number and year
\volumetitle{% Title of the volume (optional)
Proceedings of the\\
14th International Workshop on\\
Automated Verification of Critical Systems
(AVoCS 2014)}
\volumeshort{% Short title of the volume (optional)
Proc.\ AVoCS 2014}
\guesteds{% Multiple guest editors
Marieke Huisman, Jaco van de Pol}
\title{Efficacy Measurement of Early Intervention Techniques} % Title of the article
%\short{} % Short title of the article (optional)
\author{% Authors and references to addresses
Dave Donaghy\autref{1} and
Tom Crick\autref{2}}
\institute{% Institutes with labels
\autlabel{1} \email{dave.donaghy@hp.com}\\
HP Bristol, UK\par
\autlabel{2} \email{tcrick@cardiffmet.ac.uk}\\
Department of Computing\\
Cardiff Metropolitan University, UK}
\abstract{% Abstract of the article
Compiler technology has, for some considerable time, been sufficiently
advanced that individual programmers are able to produce, in
reasonably short periods of time, tools that might aid with the
development process in novel ways: for example, one can easily produce
a C compiler tool that will detect uncommon uses of integer arithmetic
(such as the rare multiplication of values that are commonly only
added) and flag such uses as potential errors.

However, there is currently no convenient way to measure the efficacy
of such techniques: where one might \begin{em}assume\end{em} that
uncommon uses of integer arithmetic \begin{em}might\end{em} be
erroneous, we do not have a way of measuring the cost saving
associated with the potential early detection of occurrences of such
things.

We present a method of measuring the efficacy of a
single \begin{em}early intervention\end{em}, based on the replaying of
previous executions of a compile-build-test cycle.  This measurement
process allows us to identify the software errors that were introduced
during an original development and subsequently fixed; additionally,
it allows us to identify the subset of such errors that would have
been identified by the early intervention. By these means, we can take
an existing historical record of a development, and extract from it
meaningful information about the value of a proposed new early
intervention technique.
}
\keywords{Verification, Software Engineering, Efficiency, Version Control
Systems, Repository Mining} % Keywords for the article

\begin{document}
\maketitle

% Main part of your article
% =========================
\section{Introduction}

It is possible for software developers to utilise all manner of tools
that will analyse practically any aspect of their source code and
report on it in any way they choose; while this freedom will allow
arbitrary invention on the part of the software developer, it might
also allow construction of analysis tools that \begin{em}seem\end{em}
effective, but in reality are not; additionally, individual developers
might have different ideas of what constitutes an effective
tool~\cite{zaidman-et-al:2008}. It would be useful, therefore, to formalise two
separate ideas:

\begin{enumerate}
\item What constitutes effectiveness in the realm of compilation tools?
\item Is my tool effective?
\end{enumerate}

\section{Robust Efficacy Measurement}

Some notions of tool effectiveness during code development and compilation
might be as follows:

\begin{enumerate}
\item Will my tool make my code better?
\item Since the beginning of my current project, how many errors would
have been detected using my tool, which was not in use (or in fact
conceived) at the time?
\end{enumerate}

Clearly these two criteria have been written deliberately to highlight
the difference between objective, measurable criteria and subjective,
hard-to-measure ones. We thus propose (and suggest ways for answering)
the following question: {\emph{how can we phrase objective questions about
measuring the effectiveness of tools run at the time of software
compilation?}}

\section{Retrospective Measurements}

We often have access to massive amounts of historical data in the form
of a source-code
repository~\cite{ball-et-al:1997,zimmermann-et-al:2005}, such as those
used by Subversion or Git (among many other tools used for such
purposes). Often, though, while these repositories provide the ability
to take time-based snapshots of workspaces that ``work'' (in some
loose sense) at a given point in time, we may not take full advantage
of the information in them~\cite{bird-et-al:2009,sisman+kak:2012}.

Imagine that we have access to a new compiler option that will allow
us to simply prevent compilation of code where a certain kind of error
is detected. Assuming we have access to full historical information in
our source-code repository, perhaps from repository mining, we can
simply re-run our compile-test cycle with the new tool in place, and
see which errors would have been identified before their actual
detection time.  Again, with full historical information, we can
identify \begin{em}how much earlier\end{em} each would have been
identified, and then make judgments about the benefits of the new
tool.

The remaining question, then, is this: if we \begin{em}do not\end{em} have
full historical information (and it is likely that we do not), then how
much information do we need to judge the efficacy of our new techniques?

\section{Open Questions}

The following questions may be answered by constructing suitable
experiments with data from historical repositories:

\begin{enumerate}
\item What build system changes are necessary to allow the retrospective
addition of compiler tools to the build cycle?

\item What artefacts must be stored in a repository in order to reproduce
compile-build test cycles and identify all errors?

\item How often must snapshots be taken in order to identify the points
in time where errors were identified and fixed?

\item What is the potential for false alarms raised by error-checking
  tools in this proposed system?

\end{enumerate}


% Acknowledgements for colleagues, referees, ...
% ==============================================
%\begin{acknowledge}
%\end{acknowledge}

% Bibliography with BibTeX
% ========================
\bibliographystyle{eceasst}
\bibliography{avocs}

\end{document}
